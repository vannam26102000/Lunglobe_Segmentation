{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as dataset_torch\n",
    "from abc import ABC\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "import time\n",
    "import numpy as np\n",
    "from lungmask import utils\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from lungmask import mask\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_extensions = [\n",
    "    '.nii.gz',\n",
    "]\n",
    "roi_extensions = [\n",
    "    '.roi.nii.gz',\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filename, mode='data'):\n",
    "    if mode == 'roi':\n",
    "        return any(filename.endswith(extension) for extension in roi_extensions)\n",
    "    elif mode == 'data':\n",
    "        if not 'roi' in filename and \\\n",
    "                any(filename.endswith(extension) for extension in data_extensions):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        raise ValueError('Undefined mode %s while reading data' % mode)\n",
    "\n",
    "\n",
    "def make_dataset(dir, max_dataset_size=float(\"inf\"), mode='data'):\n",
    "    images = []\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "\n",
    "    for root, _, fnames in sorted(os.walk(dir)):\n",
    "        # return of os.walk: root dir, folders, files\n",
    "        for fname in fnames:\n",
    "            if is_image_file(fname, mode):\n",
    "                path = os.path.join(root, fname)\n",
    "                images.append(path)\n",
    "    return images[:min(max_dataset_size, len(images))]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mget_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munet\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLTRCLobes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = mask.get_model('unet','LTRCLobes')\n",
    "model.to('cuda')\n",
    "\n",
    "summary(model, (1,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(dataset_torch.Dataset, ABC):\n",
    "    def __init__(self, dir):\n",
    "        \"\"\"\n",
    "        dir: File directory.\n",
    "        \"\"\"\n",
    "        self.dir = dir\n",
    "        self.img_list = sorted(make_dataset(dir, mode='data'))\n",
    "        self.roi_list = sorted(make_dataset(dir, mode='roi'))\n",
    "        \n",
    "        self.A_size = len(self.img_list)  # get the size of dataset\n",
    "        self.B_size = len(self.roi_list)  # get the size of roi-set\n",
    "        \n",
    "        assert(self.A_size == self.B_size)\n",
    "        if self.A_size == 0:\n",
    "            raise(RuntimeError(\"Found 0 datafiles in: \" + dir))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a data point and its metadata information.\n",
    "\n",
    "        Parameters:\n",
    "            index (int)      -- a random integer for data indexing\n",
    "\n",
    "        Returns a dictionary that contains A, B, A_paths and B_paths\n",
    "            A (tensor)       -- an image in the input domain\n",
    "            B (tensor)       -- its corresponding image in the target domain\n",
    "            A_paths (str)    -- image paths\n",
    "            B_paths (str)    -- image paths\n",
    "        \"\"\"\n",
    "        A_path = self.img_list[index] #% self.A_size]  # make sure index is within then range\n",
    "        B_path = self.roi_list[index] #% self.B_size]  # make sure index is within then range\n",
    "\n",
    "        # apply image transformation\n",
    "        A = sitk.ReadImage(A_path)  # data\n",
    "        B = sitk.ReadImage(B_path, sitk.sitkUInt8)  # roi\n",
    "\n",
    "        return {'A': A, 'B': B, 'A_paths': A_path, 'B_paths': B_path}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of images in the dataset.\n",
    "\n",
    "        As we have two datasets with potentially different number of images,\n",
    "        we take a maximum of\n",
    "        \"\"\"\n",
    "        return max(self.A_size, self.B_size)\n",
    "\n",
    "class LungLabelsDS_inf(dataset_torch.Dataset):\n",
    "    def __init__(self, ds, lb):\n",
    "        self.dataset = ds\n",
    "        self.label = lb\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx, None, :, :], self.label[idx, None, :, :]\n",
    "    \n",
    "def preprocess(img_data, roi_data):\n",
    "    img_raw = sitk.GetArrayFromImage(img_data)\n",
    "    roi_raw = sitk.GetArrayFromImage(roi_data)\n",
    "    directions = np.asarray(img_data.GetDirection())\n",
    "    if len(directions) == 9:\n",
    "        img_raw = np.flip(img_raw, np.where(directions[[0, 4, 8]][::-1] < 0)[0])\n",
    "        roi_raw = np.flip(roi_raw, np.where(directions[[0, 4, 8]][::-1] < 0)[0])\n",
    "\n",
    "    tvolslices, labelslices, xnew_box = utils.preprocess(img_raw, label=roi_raw, resolution=[256, 256])\n",
    "    \n",
    "    tvolslices = np.divide((tvolslices + 1024), 1624)\n",
    "    \n",
    "    torch_ds_val = LungLabelsDS_inf(tvolslices, xnew_box)\n",
    "    \n",
    "    return torch.utils.data.DataLoader(torch_ds_val, batch_size=16, shuffle=True, num_workers=1, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = mask.get_model('unet','LTRCLobes')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "learning_rate_decay = [500, 750]\n",
    "\n",
    "lr_decay = torch.optim.lr_scheduler.MultiStepLR(optimizer, learning_rate_decay)\n",
    "\n",
    "dir = r'/home/avitech-pc4/Nam/data'\n",
    "dataset = BaseDataset(dir)\n",
    "\n",
    "total_iters = 0\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    mean_loss = []\n",
    "    lr_decay.step()\n",
    "\n",
    "    for _, data in enumerate(dataset):\n",
    "\n",
    "        dataloader_val = preprocess(data['A'], data['B'])\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for step, (X, Y) in enumerate(dataloader_val):\n",
    "\n",
    "            X = X.float().to(device)\n",
    "            label = torch.tensor(Y[:,0,:,:],dtype=torch.long).to(device)\n",
    "            \n",
    "            prediction = model(X)\n",
    "                \n",
    "            loss = criterion(prediction, label)\n",
    "            \n",
    "            mean_loss.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "#             if step % 5 == 0:\n",
    "#                 print('epoch:{}, step:{}, loss:{:.3f}'.format(epoch, step, loss.item()))\n",
    "                      \n",
    "    mean_loss = sum(mean_loss) / len(mean_loss) \n",
    "    \n",
    "    print('epoch:{}, mean_loss:{:.3f}'.format(epoch, mean_loss))\n",
    "    \n",
    "    if epoch % 50 == 0 and epoch != 0:\n",
    "        torch.save(model.state_dict(), './checkpoint/net{}-{:.3f}-{:.3f}.pth'.format(epoch, loss, mean_loss))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
